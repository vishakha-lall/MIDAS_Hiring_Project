{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Class Classification Problem\n",
    "\n",
    "The task of predicting flairs is a multi class classification task. The flairs are **mutually exclusive**. This classification is based on the assumption that each submission is assigned to a **single flair** - which is how Reddit flairs are assigned. Each post has a single flair attached to it. \n",
    "\n",
    "#### Aim\n",
    "\n",
    "Build a flair detector - a supervised classifier using dataset scraped from Reddit */r/india* subreddit.  \n",
    "\n",
    "#### What does this code block do?\n",
    "\n",
    "Prepares the dataset to be fed to models:\n",
    "\n",
    "- Get cleaned data (from previous section)\n",
    "- Combine text data (title, selftext and comments) - Give higher weight to title and selftext over comments (Discussed in EDA)\n",
    "- Transform into input (text) and output (flair - one hot encoded)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flair</th>\n",
       "      <th>text</th>\n",
       "      <th>/r/all</th>\n",
       "      <th>40 Martyrs</th>\n",
       "      <th>AMA</th>\n",
       "      <th>AskIndia</th>\n",
       "      <th>Business/Finance</th>\n",
       "      <th>CAA-NRC</th>\n",
       "      <th>CAA-NRC-NPR</th>\n",
       "      <th>Coronavirus</th>\n",
       "      <th>...</th>\n",
       "      <th>Politics -- Source in comments</th>\n",
       "      <th>Politics [Megathread]</th>\n",
       "      <th>Scheduled</th>\n",
       "      <th>Science/Technology</th>\n",
       "      <th>Sports</th>\n",
       "      <th>Totally real</th>\n",
       "      <th>Unverified</th>\n",
       "      <th>Zoke Tyme</th>\n",
       "      <th>[R]eddiquette</th>\n",
       "      <th>r/all</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Coronavirus</td>\n",
       "      <td>coronavirus covid-19 megathread news update 4 ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Scheduled</td>\n",
       "      <td>monthly happiness thread randians share good/p...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Photography</td>\n",
       "      <td>aerial view gangaikonda cholapuram temple aeri...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Non-Political</td>\n",
       "      <td>fir arnab goswami chhattisgarh create animosit...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Coronavirus</td>\n",
       "      <td>lockdown scene kurnool andhra pradesh 203 case...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           flair                                               text  /r/all  \\\n",
       "0    Coronavirus  coronavirus covid-19 megathread news update 4 ...       0   \n",
       "1      Scheduled  monthly happiness thread randians share good/p...       0   \n",
       "2    Photography  aerial view gangaikonda cholapuram temple aeri...       0   \n",
       "3  Non-Political  fir arnab goswami chhattisgarh create animosit...       0   \n",
       "4    Coronavirus  lockdown scene kurnool andhra pradesh 203 case...       0   \n",
       "\n",
       "   40 Martyrs  AMA  AskIndia  Business/Finance  CAA-NRC  CAA-NRC-NPR  \\\n",
       "0           0    0         0                 0        0            0   \n",
       "1           0    0         0                 0        0            0   \n",
       "2           0    0         0                 0        0            0   \n",
       "3           0    0         0                 0        0            0   \n",
       "4           0    0         0                 0        0            0   \n",
       "\n",
       "   Coronavirus  ...  Politics -- Source in comments  Politics [Megathread]  \\\n",
       "0            1  ...                               0                      0   \n",
       "1            0  ...                               0                      0   \n",
       "2            0  ...                               0                      0   \n",
       "3            0  ...                               0                      0   \n",
       "4            1  ...                               0                      0   \n",
       "\n",
       "   Scheduled  Science/Technology  Sports  Totally real  Unverified  Zoke Tyme  \\\n",
       "0          0                   0       0             0           0          0   \n",
       "1          1                   0       0             0           0          0   \n",
       "2          0                   0       0             0           0          0   \n",
       "3          0                   0       0             0           0          0   \n",
       "4          0                   0       0             0           0          0   \n",
       "\n",
       "   [R]eddiquette  r/all  \n",
       "0              0      0  \n",
       "1              0      0  \n",
       "2              0      0  \n",
       "3              0      0  \n",
       "4              0      0  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_pickle('submissions_df_clean.pkl')\n",
    "\n",
    "dataset = dataset[['flair','title_processed','comments_processed','selftext_processed']]\n",
    "dataset['text'] = 3*dataset['title_processed']+2*dataset['selftext_processed']+dataset['comments_processed']\n",
    "dataset['text'] = dataset['text'].apply(lambda x: ' '.join([str(elem) for elem in x]))\n",
    "dataset = dataset[['flair','text']]\n",
    "\n",
    "dataset = dataset.assign(**pd.get_dummies(dataset['flair']))\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classes\n",
    "\n",
    "The submissions will be classified in the top 10 flairs (Discussed in EDA). All the other submissions - with flairs other than the `top_flairs` are classified as *Others*. \n",
    "\n",
    "#### What does this code block do?\n",
    "\n",
    "Get the list of top flairs from previous section as a list. These will serve as our distinct classes of the multi-class classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Non-Political',\n",
       " 'Politics',\n",
       " 'Coronavirus',\n",
       " 'AskIndia',\n",
       " 'Policy/Economy',\n",
       " 'Business/Finance',\n",
       " 'Photography',\n",
       " '[R]eddiquette',\n",
       " 'Sports',\n",
       " 'Science/Technology',\n",
       " 'Others']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_flairs = pd.read_pickle('top_flairs.pkl')\n",
    "\n",
    "flairs = top_flairs.index.to_list()\n",
    "flairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What does this code block do?\n",
    "\n",
    "- Transform dataset to allowed classes\n",
    "- Remove flairs except the ones in `top_flairs`, assign class *Others* to such records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flair</th>\n",
       "      <th>text</th>\n",
       "      <th>Non-Political</th>\n",
       "      <th>Politics</th>\n",
       "      <th>Coronavirus</th>\n",
       "      <th>AskIndia</th>\n",
       "      <th>Policy/Economy</th>\n",
       "      <th>Business/Finance</th>\n",
       "      <th>Photography</th>\n",
       "      <th>[R]eddiquette</th>\n",
       "      <th>Sports</th>\n",
       "      <th>Science/Technology</th>\n",
       "      <th>Others</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Coronavirus</td>\n",
       "      <td>coronavirus covid-19 megathread news update 4 ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Scheduled</td>\n",
       "      <td>monthly happiness thread randians share good/p...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Photography</td>\n",
       "      <td>aerial view gangaikonda cholapuram temple aeri...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Non-Political</td>\n",
       "      <td>fir arnab goswami chhattisgarh create animosit...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Coronavirus</td>\n",
       "      <td>lockdown scene kurnool andhra pradesh 203 case...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           flair                                               text  \\\n",
       "0    Coronavirus  coronavirus covid-19 megathread news update 4 ...   \n",
       "1      Scheduled  monthly happiness thread randians share good/p...   \n",
       "2    Photography  aerial view gangaikonda cholapuram temple aeri...   \n",
       "3  Non-Political  fir arnab goswami chhattisgarh create animosit...   \n",
       "4    Coronavirus  lockdown scene kurnool andhra pradesh 203 case...   \n",
       "\n",
       "   Non-Political  Politics  Coronavirus  AskIndia  Policy/Economy  \\\n",
       "0              0         0            1         0               0   \n",
       "1              0         0            0         0               0   \n",
       "2              0         0            0         0               0   \n",
       "3              1         0            0         0               0   \n",
       "4              0         0            1         0               0   \n",
       "\n",
       "   Business/Finance  Photography  [R]eddiquette  Sports  Science/Technology  \\\n",
       "0                 0            0              0       0                   0   \n",
       "1                 0            0              0       0                   0   \n",
       "2                 0            1              0       0                   0   \n",
       "3                 0            0              0       0                   0   \n",
       "4                 0            0              0       0                   0   \n",
       "\n",
       "   Others  \n",
       "0       0  \n",
       "1       1  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset[['flair','text']+flairs[:-1]].assign(Others=dataset[dataset.columns.difference(flairs[:-1])].max(1))\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What does this code block do?\n",
    "\n",
    "- Split into testing and training data.\n",
    "- Given the small volume of data, to maximize learning I have gone ahead with a 80:20 train:test split.\n",
    "- For the more advanced models, this 20% will serve as validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size: Input: (1389,) Output: (1389, 11)\n",
      "Training data size: Input: (348,) Output: (348, 11)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset['text'], dataset.loc[:, ~dataset.columns.isin(['flair', 'text'])], test_size=0.20, random_state=42)\n",
    "print(\"Training data size: Input: \"+str(X_train.shape)+\" Output: \"+str(y_train.shape))\n",
    "print(\"Training data size: Input: \"+str(X_test.shape)+\" Output: \"+str(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How I chose to proceed?\n",
    "\n",
    "To begin with, I started with some basic binary classification models trained independently for the flairs to get a sense of complexity of the data and a motivation to proceed with some more advanced models. \n",
    "\n",
    "#### Understanding the Classifier code for the next few blocks\n",
    "\n",
    "- **Pipeline** - to automate the workflow (manipulations and transformations)\n",
    "\n",
    "- The multi class algorithm accepts a **binary mask** over multiple flairs. The result for each prediction will be an array of 0s and 1s marking which flair apply to each row input sample.\n",
    "\n",
    "- Vectorizer - The next few blocks of code use the popular TF IDF vectorizer (which is independent of our corpus, hence picked for the simpler models) - to systematically compute word counts using **CountVectorizer **and then compute the **Inverse Document Frequency** (IDF) values and only then compute the Tf-idf scores. \n",
    "\n",
    " \n",
    "\n",
    "#### What does this code block do?\n",
    "\n",
    "- Classifier - **Binary Naive Bayes Classifier** - MultinomialNB \n",
    "\n",
    "- Vectorizer - TF IDF vectorizer \n",
    "\n",
    "- OneVsRestClassifier - to wrap for multi class classification\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Processing Non-Political\n",
      "Test accuracy is 0.6609195402298851\n",
      "... Processing Politics\n",
      "Test accuracy is 0.8103448275862069\n",
      "... Processing Coronavirus\n",
      "Test accuracy is 0.8017241379310345\n",
      "... Processing AskIndia\n",
      "Test accuracy is 0.9339080459770115\n",
      "... Processing Policy/Economy\n",
      "Test accuracy is 0.9511494252873564\n",
      "... Processing Business/Finance\n",
      "Test accuracy is 0.9770114942528736\n",
      "... Processing Photography\n",
      "Test accuracy is 0.9626436781609196\n",
      "... Processing [R]eddiquette\n",
      "Test accuracy is 0.9885057471264368\n",
      "... Processing Sports\n",
      "Test accuracy is 0.9885057471264368\n",
      "... Processing Science/Technology\n",
      "Test accuracy is 0.9885057471264368\n",
      "... Processing Others\n",
      "Test accuracy is 0.9396551724137931\n"
     ]
    }
   ],
   "source": [
    "NB_pipeline = Pipeline([('tfidf', TfidfVectorizer(stop_words=stop_words)), ('clf', OneVsRestClassifier(MultinomialNB(fit_prior=True, class_prior=None))),])\n",
    "for flair in flairs:\n",
    "    print('... Processing '+str(flair))\n",
    "    NB_pipeline.fit(X_train, y_train[flair])\n",
    "    prediction = NB_pipeline.predict(X_test)\n",
    "    print('Test accuracy is '+str(accuracy_score(y_test[flair], prediction)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What does this code block do?\n",
    "\n",
    "- Classifier - **Binary Linear SVC Classifier** - LinearSVC \n",
    "- Vectorizer - TF IDF vectorizer \n",
    "- OneVsRestClassifier - to wrap for multi class classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Processing Non-Political\n",
      "Test accuracy is 0.7931034482758621\n",
      "... Processing Politics\n",
      "Test accuracy is 0.8994252873563219\n",
      "... Processing Coronavirus\n",
      "Test accuracy is 0.8706896551724138\n",
      "... Processing AskIndia\n",
      "Test accuracy is 0.9425287356321839\n",
      "... Processing Policy/Economy\n",
      "Test accuracy is 0.9482758620689655\n",
      "... Processing Business/Finance\n",
      "Test accuracy is 0.9770114942528736\n",
      "... Processing Photography\n",
      "Test accuracy is 0.9655172413793104\n",
      "... Processing [R]eddiquette\n",
      "Test accuracy is 0.9885057471264368\n",
      "... Processing Sports\n",
      "Test accuracy is 0.9971264367816092\n",
      "... Processing Science/Technology\n",
      "Test accuracy is 0.9885057471264368\n",
      "... Processing Others\n",
      "Test accuracy is 0.9511494252873564\n"
     ]
    }
   ],
   "source": [
    "SVC_pipeline = Pipeline([('tfidf', TfidfVectorizer(stop_words=stop_words)), ('clf', OneVsRestClassifier(LinearSVC(), n_jobs=1)),])\n",
    "for flair in flairs:\n",
    "    print('... Processing '+str(flair))\n",
    "    SVC_pipeline.fit(X_train, y_train[flair])\n",
    "    prediction = SVC_pipeline.predict(X_test)\n",
    "    print('Test accuracy is '+str(accuracy_score(y_test[flair], prediction)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What does this code block do?\n",
    "\n",
    "- Classifier - **Binary Logistic Regression Classifier** - LogisticRegression \n",
    "- Vectorizer - TF IDF vectorizer \n",
    "- OneVsRestClassifier - to wrap for multi class classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Processing Non-Political\n",
      "Test accuracy is 0.7442528735632183\n",
      "... Processing Politics\n",
      "Test accuracy is 0.8706896551724138\n",
      "... Processing Coronavirus\n",
      "Test accuracy is 0.8419540229885057\n",
      "... Processing AskIndia\n",
      "Test accuracy is 0.9339080459770115\n",
      "... Processing Policy/Economy\n",
      "Test accuracy is 0.9511494252873564\n",
      "... Processing Business/Finance\n",
      "Test accuracy is 0.9770114942528736\n",
      "... Processing Photography\n",
      "Test accuracy is 0.9626436781609196\n",
      "... Processing [R]eddiquette\n",
      "Test accuracy is 0.9885057471264368\n",
      "... Processing Sports\n",
      "Test accuracy is 0.9885057471264368\n",
      "... Processing Science/Technology\n",
      "Test accuracy is 0.9885057471264368\n",
      "... Processing Others\n",
      "Test accuracy is 0.9396551724137931\n"
     ]
    }
   ],
   "source": [
    "LogReg_pipeline = Pipeline([('tfidf', TfidfVectorizer(stop_words=stop_words)),('clf', OneVsRestClassifier(LogisticRegression(solver='sag'), n_jobs=1)),])\n",
    "for flair in flairs:\n",
    "    print('... Processing '+str(flair))\n",
    "    LogReg_pipeline.fit(X_train, y_train[flair])\n",
    "    prediction = LogReg_pipeline.predict(X_test)\n",
    "    print('Test accuracy is '+str(accuracy_score(y_test[flair], prediction)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What did I observe?\n",
    "\n",
    "At first the excellent accuracy values might deceive one that the models are performing great, however, it's worthy to note that the above models are binary classifiers trained for each individual flair. As a result while the accuracy for *Science/Technology* is 98% it is not a reasonable representation of accuracy. In other words, the classifier tests whether a submission is *Science/Technology* or not and not whether is *Science/Technology* or *Political* or any other flair.  \n",
    "\n",
    "> I tried this by testing the model with some randomly picked text from /r/india. These models performs identically and poorly. The text was classified as Science/Technology and Political with similar probabilities. \n",
    "\n",
    "#### What next?\n",
    "\n",
    "I decided to move on to basic but multi class models that were not binary in nature.\n",
    "\n",
    "##### Random Forest Classifier\n",
    "\n",
    "- vectorizer - TF IDF vectorizer\n",
    "- RandomForestClassifier - Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vishakha Lall\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score 0.27011494252873564\n",
      "F1 Score (Micro) 0.3900414937759336\n",
      "F1 Score (Weighted) 0.34871874756600985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vishakha Lall\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "RF_pipeline = Pipeline([('tfidf', TfidfVectorizer(stop_words=stop_words)), ('clf', RandomForestClassifier())])\n",
    "RF_pipeline.fit(X_train, y_train)\n",
    "print(\"Accuracy Score \"+str(accuracy_score(y_test, RF_pipeline.predict(X_test))))\n",
    "print(\"F1 Score (Micro) \"+str(f1_score(y_test, RF_pipeline.predict(X_test), average='micro')))\n",
    "print(\"F1 Score (Weighted) \"+str(f1_score(y_test, RF_pipeline.predict(X_test), average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Linear SVC Classifier\n",
    "\n",
    "- vectorizer - TF IDF vectorizer\n",
    "- LinearSVC - Linear SVC Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score 0.6551724137931034\n",
      "F1 Score (Micro) 0.6551724137931034\n",
      "F1 Score (Weighted) 0.6098445344943171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vishakha Lall\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "LSVC_pipeline = Pipeline([('tfidf', TfidfVectorizer(stop_words=stop_words)), ('clf', LinearSVC())])\n",
    "LSVC_pipeline.fit(X_train, np.argmax(np.array(y_train), axis=1))\n",
    "print(\"Accuracy Score \"+str(accuracy_score(np.argmax(np.array(y_test), axis=1), LSVC_pipeline.predict(X_test))))\n",
    "print(\"F1 Score (Micro) \"+str(f1_score(np.argmax(np.array(y_test), axis=1), LSVC_pipeline.predict(X_test), average='micro')))\n",
    "print(\"F1 Score (Weighted) \"+str(f1_score(np.argmax(np.array(y_test), axis=1), LSVC_pipeline.predict(X_test), average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What did I observe?\n",
    "\n",
    "The poor accuracy results from these models suggest the complexity of data and how more complex neural network models are required to work with such data. This is backed not only by the results of these models, but also by popular research in the field of RNNs for text based data.\n",
    "\n",
    "##### LSTM Model\n",
    "\n",
    "Motivation behind choosing the model\n",
    "- LSTM outperforms other models on text data when we want our model to learn from long term dependencies. LSTM’s ability to forget, remember and update the information pushes it one step ahead of RNNs.\n",
    "- LSTM is a popular step in to begin with advanced models\n",
    "\n",
    "Steps\n",
    "- Tokenize text to embedded vectors\n",
    "- Build LSTM\n",
    "    - `embed_dim` : The embedding layer encodes the input sequence into a sequence of dense vectors of dimension embed_dim.\n",
    "    - `lstm_out` : The LSTM transforms the vector sequence into a single vector of size lstm_out, containing information about the entire sequence.\n",
    "    - 'softmax'activation function\n",
    "- Fit on training data, check accuracy on validation set\n",
    "\n",
    "Resource for [LSTM](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence, text\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Input, LSTM\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.callbacks import EarlyStopping\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Converting text to vector\n",
    "\n",
    "In the previous models, TF ID vectorization encoding has been used to convert text to vector. This section explores a tokenizer that is fit on the corpus of the submissions (specific to our dataset).  \n",
    "\n",
    "The test and train dataset are then converted to vector using this tokenizer.\n",
    "\n",
    "`pad_sequences` transforms the vector into a 2D Numpy array of shape (*length_of_vector* x 300) to be used in the RNN. The motivation to select 300 is that it is close to the average length of text in words.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer()\n",
    "tokenizer.fit_on_texts(dataset['text'])\n",
    "with open('tokenizer.pkl', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "X_train_2 = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_2 = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "X_train_sequence = sequence.pad_sequences(X_train_2, maxlen=300)\n",
    "X_test_sequence = sequence.pad_sequences(X_test_2, maxlen=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find average count of words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "306.3615428900403"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['word_count'] = dataset['text'].apply(lambda x: len(x.split()))\n",
    "dataset['word_count'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(348,)\n",
      "(348, 300)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ..., 1336, 4728,  227],\n",
       "       [ 148, 2223,   41, ...,  540,  429,   86],\n",
       "       [   0,    0,    0, ...,   18, 4134,  226],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,  298,  125, 7303],\n",
       "       [   0,    0,    0, ...,   18, 2052,  715],\n",
       "       [   0,    0,    0, ...,   85,  522, 1436]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X_test.shape)\n",
    "print(X_test_sequence.shape)\n",
    "X_test_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Architecture \n",
    "one input layer, one embedding layer, one LSTM layer with 200 neurons and one output layer with 11 neurons since we have 11 flairs in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Vishakha Lall\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From C:\\Users\\Vishakha Lall\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 1389 samples, validate on 348 samples\n",
      "Epoch 1/25\n",
      "1389/1389 [==============================] - 35s 25ms/step - loss: 2.0710 - accuracy: 0.2966 - val_loss: 1.9001 - val_accuracy: 0.2443\n",
      "Epoch 2/25\n",
      "1389/1389 [==============================] - 34s 24ms/step - loss: 1.8513 - accuracy: 0.3125 - val_loss: 1.8708 - val_accuracy: 0.3851\n",
      "Epoch 3/25\n",
      "1389/1389 [==============================] - 34s 25ms/step - loss: 1.5549 - accuracy: 0.5472 - val_loss: 1.6739 - val_accuracy: 0.4770\n",
      "Epoch 4/25\n",
      "1389/1389 [==============================] - 34s 24ms/step - loss: 0.9179 - accuracy: 0.7307 - val_loss: 1.7561 - val_accuracy: 0.5316\n",
      "Epoch 5/25\n",
      "1389/1389 [==============================] - 34s 24ms/step - loss: 0.5915 - accuracy: 0.8157 - val_loss: 1.6907 - val_accuracy: 0.5144\n",
      "Epoch 6/25\n",
      "1389/1389 [==============================] - 37s 26ms/step - loss: 0.4096 - accuracy: 0.8711 - val_loss: 1.8605 - val_accuracy: 0.5086\n",
      "Epoch 7/25\n",
      "1389/1389 [==============================] - 34s 25ms/step - loss: 0.2751 - accuracy: 0.9316 - val_loss: 1.9867 - val_accuracy: 0.4511\n",
      "Epoch 8/25\n",
      "1389/1389 [==============================] - 37s 27ms/step - loss: 0.1717 - accuracy: 0.9525 - val_loss: 2.0375 - val_accuracy: 0.4655\n",
      "Epoch 9/25\n",
      "1389/1389 [==============================] - 35s 25ms/step - loss: 0.1262 - accuracy: 0.9618 - val_loss: 1.9985 - val_accuracy: 0.4138\n",
      "Epoch 10/25\n",
      "1389/1389 [==============================] - 34s 25ms/step - loss: 0.0696 - accuracy: 0.9798 - val_loss: 2.1430 - val_accuracy: 0.4483\n",
      "Epoch 11/25\n",
      "1389/1389 [==============================] - 34s 25ms/step - loss: 0.0597 - accuracy: 0.9914 - val_loss: 2.0568 - val_accuracy: 0.4569\n",
      "Epoch 12/25\n",
      "1389/1389 [==============================] - 33s 24ms/step - loss: 0.0409 - accuracy: 0.9892 - val_loss: 2.1754 - val_accuracy: 0.4713\n",
      "Epoch 13/25\n",
      "1389/1389 [==============================] - 34s 24ms/step - loss: 0.0204 - accuracy: 0.9957 - val_loss: 2.2503 - val_accuracy: 0.4224\n",
      "Epoch 14/25\n",
      "1389/1389 [==============================] - 34s 25ms/step - loss: 0.0151 - accuracy: 0.9971 - val_loss: 2.3173 - val_accuracy: 0.4310\n",
      "Epoch 15/25\n",
      "1389/1389 [==============================] - 35s 25ms/step - loss: 0.0079 - accuracy: 0.9964 - val_loss: 2.4231 - val_accuracy: 0.4397\n",
      "Epoch 16/25\n",
      "1389/1389 [==============================] - 36s 26ms/step - loss: 0.0045 - accuracy: 0.9978 - val_loss: 2.4181 - val_accuracy: 0.4310\n",
      "Epoch 17/25\n",
      "1389/1389 [==============================] - 35s 25ms/step - loss: 0.0024 - accuracy: 0.9978 - val_loss: 2.5410 - val_accuracy: 0.4253\n",
      "Epoch 18/25\n",
      "1389/1389 [==============================] - 35s 25ms/step - loss: 0.0017 - accuracy: 0.9978 - val_loss: 2.5959 - val_accuracy: 0.4339\n",
      "Epoch 19/25\n",
      "1389/1389 [==============================] - 34s 24ms/step - loss: 0.0846 - accuracy: 0.9791 - val_loss: 2.1516 - val_accuracy: 0.4138\n",
      "Epoch 20/25\n",
      "1389/1389 [==============================] - 36s 26ms/step - loss: 0.0334 - accuracy: 0.9892 - val_loss: 2.3003 - val_accuracy: 0.4052\n",
      "Epoch 21/25\n",
      "1389/1389 [==============================] - 35s 25ms/step - loss: 0.0190 - accuracy: 0.9950 - val_loss: 2.4953 - val_accuracy: 0.3822\n",
      "Epoch 22/25\n",
      "1389/1389 [==============================] - 36s 26ms/step - loss: 0.0093 - accuracy: 0.9964 - val_loss: 2.3874 - val_accuracy: 0.4511\n",
      "Epoch 23/25\n",
      "1389/1389 [==============================] - 35s 25ms/step - loss: 0.0096 - accuracy: 0.9957 - val_loss: 2.5664 - val_accuracy: 0.4224\n",
      "Epoch 24/25\n",
      "1389/1389 [==============================] - 34s 25ms/step - loss: 0.0030 - accuracy: 0.9978 - val_loss: 2.5452 - val_accuracy: 0.4224\n",
      "Epoch 25/25\n",
      "1389/1389 [==============================] - 33s 24ms/step - loss: 0.0016 - accuracy: 0.9971 - val_loss: 2.5854 - val_accuracy: 0.4167\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x201f7a91e88>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_length = 200\n",
    "model = Sequential()\n",
    "model.add(Embedding( len(tokenizer.word_index)+1, embedding_length ,input_length = X_train_sequence.shape[1]))\n",
    "model.add(LSTM(embedding_length, dropout=0.2))\n",
    "model.add(Dense(11,activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "\n",
    "model.fit(X_train_sequence, np.array(y_train), batch_size=64,epochs=25,\n",
    "          validation_data=(X_test_sequence, np.array(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modifying sequence length to 80\n",
    "\n",
    "Based on observations, the accuracy improved with smaller values of `maxlen`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1389 samples, validate on 348 samples\n",
      "Epoch 1/25\n",
      "1389/1389 [==============================] - 12s 9ms/step - loss: 2.1211 - accuracy: 0.2743 - val_loss: 1.9112 - val_accuracy: 0.1925\n",
      "Epoch 2/25\n",
      "1389/1389 [==============================] - 11s 8ms/step - loss: 1.8555 - accuracy: 0.3189 - val_loss: 1.8693 - val_accuracy: 0.3736\n",
      "Epoch 3/25\n",
      "1389/1389 [==============================] - 11s 8ms/step - loss: 1.6577 - accuracy: 0.5349 - val_loss: 1.8179 - val_accuracy: 0.4626\n",
      "Epoch 4/25\n",
      "1389/1389 [==============================] - 11s 8ms/step - loss: 1.1031 - accuracy: 0.6479 - val_loss: 1.6652 - val_accuracy: 0.5144\n",
      "Epoch 5/25\n",
      "1389/1389 [==============================] - 11s 8ms/step - loss: 0.6752 - accuracy: 0.7855 - val_loss: 1.7536 - val_accuracy: 0.5000\n",
      "Epoch 6/25\n",
      "1389/1389 [==============================] - 13s 9ms/step - loss: 0.4965 - accuracy: 0.8596 - val_loss: 1.9886 - val_accuracy: 0.4454\n",
      "Epoch 7/25\n",
      "1389/1389 [==============================] - 12s 9ms/step - loss: 0.3641 - accuracy: 0.9114 - val_loss: 2.0118 - val_accuracy: 0.4569\n",
      "Epoch 8/25\n",
      "1389/1389 [==============================] - 12s 9ms/step - loss: 0.2378 - accuracy: 0.9381 - val_loss: 2.1685 - val_accuracy: 0.4224\n",
      "Epoch 9/25\n",
      "1389/1389 [==============================] - 11s 8ms/step - loss: 0.1807 - accuracy: 0.9496 - val_loss: 1.9600 - val_accuracy: 0.4397\n",
      "Epoch 10/25\n",
      "1389/1389 [==============================] - 12s 8ms/step - loss: 0.0876 - accuracy: 0.9748 - val_loss: 2.2025 - val_accuracy: 0.4253\n",
      "Epoch 11/25\n",
      "1389/1389 [==============================] - 12s 9ms/step - loss: 0.0430 - accuracy: 0.9878 - val_loss: 2.3711 - val_accuracy: 0.4454\n",
      "Epoch 12/25\n",
      "1389/1389 [==============================] - 11s 8ms/step - loss: 0.0316 - accuracy: 0.9942 - val_loss: 2.1234 - val_accuracy: 0.4397\n",
      "Epoch 13/25\n",
      "1389/1389 [==============================] - 12s 8ms/step - loss: 0.0158 - accuracy: 0.9950 - val_loss: 2.3025 - val_accuracy: 0.4368\n",
      "Epoch 14/25\n",
      "1389/1389 [==============================] - 11s 8ms/step - loss: 0.0084 - accuracy: 0.9986 - val_loss: 2.3931 - val_accuracy: 0.4339\n",
      "Epoch 15/25\n",
      "1389/1389 [==============================] - 12s 8ms/step - loss: 0.0236 - accuracy: 0.9935 - val_loss: 2.2874 - val_accuracy: 0.3851\n",
      "Epoch 16/25\n",
      "1389/1389 [==============================] - 15s 11ms/step - loss: 0.0137 - accuracy: 0.9950 - val_loss: 2.6901 - val_accuracy: 0.4109\n",
      "Epoch 17/25\n",
      "1389/1389 [==============================] - 14s 10ms/step - loss: 0.0142 - accuracy: 0.9971 - val_loss: 2.4631 - val_accuracy: 0.4080\n",
      "Epoch 18/25\n",
      "1389/1389 [==============================] - 13s 10ms/step - loss: 0.0079 - accuracy: 0.9950 - val_loss: 2.4513 - val_accuracy: 0.4368\n",
      "Epoch 19/25\n",
      "1389/1389 [==============================] - 13s 9ms/step - loss: 0.0060 - accuracy: 0.9964 - val_loss: 2.4711 - val_accuracy: 0.3822\n",
      "Epoch 20/25\n",
      "1389/1389 [==============================] - 15s 10ms/step - loss: 0.0027 - accuracy: 0.9971 - val_loss: 2.5676 - val_accuracy: 0.4052\n",
      "Epoch 21/25\n",
      "1389/1389 [==============================] - 14s 10ms/step - loss: 0.0017 - accuracy: 0.9978 - val_loss: 2.6264 - val_accuracy: 0.4224\n",
      "Epoch 22/25\n",
      "1389/1389 [==============================] - 12s 9ms/step - loss: 0.0013 - accuracy: 0.9964 - val_loss: 2.7379 - val_accuracy: 0.4253\n",
      "Epoch 23/25\n",
      "1389/1389 [==============================] - 12s 9ms/step - loss: 0.0011 - accuracy: 0.9964 - val_loss: 2.8083 - val_accuracy: 0.4080\n",
      "Epoch 24/25\n",
      "1389/1389 [==============================] - 12s 9ms/step - loss: 9.2150e-04 - accuracy: 0.9971 - val_loss: 2.8808 - val_accuracy: 0.4080\n",
      "Epoch 25/25\n",
      "1389/1389 [==============================] - 11s 8ms/step - loss: 9.3466e-04 - accuracy: 0.9986 - val_loss: 2.9299 - val_accuracy: 0.3937\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x201f9efc408>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_sequence = sequence.pad_sequences(X_train_2, maxlen=80)\n",
    "X_test_sequence = sequence.pad_sequences(X_test_2, maxlen=80)\n",
    "embedding_length = 200\n",
    "model = Sequential()\n",
    "model.add(Embedding( len(tokenizer.word_index)+1, embedding_length ,input_length = X_train_sequence.shape[1]))\n",
    "model.add(LSTM(embedding_length, dropout=0.2))\n",
    "model.add(Dense(11,activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "\n",
    "model.fit(X_train_sequence, np.array(y_train), batch_size=64,epochs=25,\n",
    "          validation_data=(X_test_sequence, np.array(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model_lstm.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What did I observe?\n",
    "\n",
    "A common observation is the falling of validation-accuracy with the increase in epochs. Usually, the validation metric stops improving after a certain number of epochs and begins to decrease afterward, indicating **overfitting** ie  the model learned patterns specific to the training data, which are irrelevant in other data. Some of the solutions I thought of:\n",
    "\n",
    "1. **Reduce learning rate** to a very small number like 0.001 or even 0.0001.\n",
    "2. Provide **more data**. (This is restricted with the current volume of data)\n",
    "3. Set **Dropout rates** to a number like 0.2. **Keep them uniform across the network**.\n",
    "4. Try **decreasing the batch size**.\n",
    "\n",
    "#### What next?\n",
    "\n",
    "I ran the model with early stopping to get the best accuracy possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1389 samples, validate on 348 samples\n",
      "Epoch 1/50\n",
      "1389/1389 [==============================] - 13s 9ms/step - loss: 0.0121 - accuracy: 0.9935 - val_loss: 2.4918 - val_accuracy: 0.4224\n",
      "Epoch 2/50\n",
      "1389/1389 [==============================] - 13s 9ms/step - loss: 0.0295 - accuracy: 0.9892 - val_loss: 2.2446 - val_accuracy: 0.4080\n",
      "Epoch 3/50\n",
      "1389/1389 [==============================] - 13s 9ms/step - loss: 0.0659 - accuracy: 0.9842 - val_loss: 2.3565 - val_accuracy: 0.3966\n",
      "Epoch 4/50\n",
      "1389/1389 [==============================] - 13s 9ms/step - loss: 0.0145 - accuracy: 0.9986 - val_loss: 2.5263 - val_accuracy: 0.3966\n",
      "Epoch 5/50\n",
      "1389/1389 [==============================] - 13s 9ms/step - loss: 0.0076 - accuracy: 0.9957 - val_loss: 2.5836 - val_accuracy: 0.3994\n",
      "Epoch 6/50\n",
      "1389/1389 [==============================] - 13s 9ms/step - loss: 0.0120 - accuracy: 0.9957 - val_loss: 2.3952 - val_accuracy: 0.4282\n",
      "Epoch 7/50\n",
      "1389/1389 [==============================] - 13s 9ms/step - loss: 0.0047 - accuracy: 0.9978 - val_loss: 2.5312 - val_accuracy: 0.4224\n",
      "Epoch 8/50\n",
      "1389/1389 [==============================] - 13s 9ms/step - loss: 0.0021 - accuracy: 0.9971 - val_loss: 2.6256 - val_accuracy: 0.4253\n",
      "Epoch 9/50\n",
      "1389/1389 [==============================] - 13s 9ms/step - loss: 0.0013 - accuracy: 0.9986 - val_loss: 2.6539 - val_accuracy: 0.4253\n",
      "Epoch 10/50\n",
      "1389/1389 [==============================] - 13s 9ms/step - loss: 9.7658e-04 - accuracy: 0.9986 - val_loss: 2.6755 - val_accuracy: 0.4253\n",
      "Epoch 11/50\n",
      "1389/1389 [==============================] - 14s 10ms/step - loss: 8.2269e-04 - accuracy: 0.9986 - val_loss: 2.7012 - val_accuracy: 0.4253\n",
      "Epoch 12/50\n",
      "1389/1389 [==============================] - 13s 9ms/step - loss: 7.2255e-04 - accuracy: 0.9993 - val_loss: 2.7368 - val_accuracy: 0.4282\n",
      "Epoch 13/50\n",
      "1389/1389 [==============================] - 13s 9ms/step - loss: 6.9338e-04 - accuracy: 0.9964 - val_loss: 2.7583 - val_accuracy: 0.4339\n",
      "Epoch 14/50\n",
      "1389/1389 [==============================] - 13s 9ms/step - loss: 7.8492e-04 - accuracy: 1.0000 - val_loss: 2.6626 - val_accuracy: 0.4195\n",
      "Epoch 15/50\n",
      "1389/1389 [==============================] - 13s 9ms/step - loss: 6.1850e-04 - accuracy: 0.9971 - val_loss: 2.6599 - val_accuracy: 0.4282\n",
      "Epoch 16/50\n",
      "1389/1389 [==============================] - 13s 9ms/step - loss: 6.1853e-04 - accuracy: 1.0000 - val_loss: 2.6753 - val_accuracy: 0.4224\n",
      "Epoch 17/50\n",
      "1389/1389 [==============================] - 13s 9ms/step - loss: 6.4589e-04 - accuracy: 0.9964 - val_loss: 2.6122 - val_accuracy: 0.4339\n",
      "Epoch 18/50\n",
      "1389/1389 [==============================] - 13s 9ms/step - loss: 5.3049e-04 - accuracy: 0.9986 - val_loss: 2.6666 - val_accuracy: 0.4195\n",
      "Epoch 19/50\n",
      "1389/1389 [==============================] - 13s 9ms/step - loss: 4.7466e-04 - accuracy: 0.9964 - val_loss: 2.7045 - val_accuracy: 0.4167\n",
      "Epoch 20/50\n",
      "1389/1389 [==============================] - 14s 10ms/step - loss: 4.8840e-04 - accuracy: 0.9986 - val_loss: 2.7106 - val_accuracy: 0.4224\n",
      "Epoch 21/50\n",
      "1389/1389 [==============================] - 13s 9ms/step - loss: 5.0596e-04 - accuracy: 0.9986 - val_loss: 2.7703 - val_accuracy: 0.4282\n",
      "Epoch 22/50\n",
      "1389/1389 [==============================] - 13s 9ms/step - loss: 5.3103e-04 - accuracy: 0.9978 - val_loss: 2.8777 - val_accuracy: 0.4138\n",
      "Epoch 23/50\n",
      "1389/1389 [==============================] - 13s 9ms/step - loss: 0.0017 - accuracy: 0.9971 - val_loss: 2.8385 - val_accuracy: 0.4224\n",
      "Epoch 00023: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x201edac2b88>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es = EarlyStopping(monitor='val_accuracy', mode='max', verbose=1, patience=10)\n",
    "\n",
    "model.fit(X_train_sequence, np.array(y_train), batch_size=64,epochs=50,\n",
    "          validation_data=(X_test_sequence, np.array(y_test)), callbacks=[es])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LSTM model is used to make the predictions in the web service available [here](http://http://cryptic-earth-17134.herokuapp.com/).\n",
    "\n",
    "#### **What can be improved?**\n",
    "\n",
    "- Larger dataset - currently Reddit only allows getting the top() and hot() posts which is limited to the number of records extracted, a larger data would increase the amount of data that passes through the model\n",
    "- More advanced models, possibly explore other RNN based and improvement models\n",
    "- Modifications to layers in the LSTM model, activation function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
